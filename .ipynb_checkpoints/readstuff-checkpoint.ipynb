{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "this film wa just brilliant casting location scenery story direction everyone really suited the part they played and you could just imagine being there robert redford is an amazing actor and now the same being director norman father came from the same scottish island a myself so loved the fact there wa real connection with this film the witty remark throughout the film were great it wa just brilliant so much that bought the film a soon a it wa released for retail and would recommend it to everyone to watch and the fly fishing wa amazing really cried at the end it wa so sad and you know what they say if you cry at film it must have been good and this definitely wa also congratulation to the two little boy that played the part of norman and paul they were just brilliant child are often left out of the praising list think because the star that play them all grown up are such big profile for the whole film but these child are amazing and should be praised for what they have done don you think the whole story wa so lovely because it wa true and wa someone life after all that wa shared with u all\n"
     ]
    }
   ],
   "source": [
    "## read text and preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# download when needed\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "tfile = open('imdb_train.txt','r',encoding='utf-8')\n",
    "Y = [] #label\n",
    "x = [] #initial feature\n",
    "\n",
    "#what the file looks like:\n",
    "# 1 this movie is very good .......\n",
    "for line in tfile:\n",
    "    Y.append(line[0]) #take the 1\n",
    "    x.append(line[2:])#take the text\n",
    "\n",
    "tmp = [] #storing the processed x\n",
    "for w in range(0, len(x)):\n",
    "    # remove special characters\n",
    "    txt = re.sub(r'\\W', ' ', str(x[w]))\n",
    "    \n",
    "    # remove characters\n",
    "    txt = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)\n",
    "    \n",
    "    # Remove single characters start\n",
    "    txt = re.sub(r'\\^[a-zA-Z]\\s+', ' ', txt) \n",
    "    \n",
    "    # remove multiple spaces with single space\n",
    "    txt = re.sub(r'\\s+', ' ', txt, flags=re.I)\n",
    "    \n",
    "    # removing prefixed 'b'\n",
    "    txt = re.sub(r'^b\\s+', '', txt)\n",
    "    \n",
    "    # turn lowercase\n",
    "    txt = txt.lower()\n",
    "    \n",
    "    # lemmatization\n",
    "    txt = txt.split()\n",
    "\n",
    "    txt = [wnl.lemmatize(word) for word in txt]\n",
    "    txt = ' '.join(txt)\n",
    "    \n",
    "    tmp.append(txt)\n",
    "    \n",
    "print(len(tmp))\n",
    "print(tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## turn text into num with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "16\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "5\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "15\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize vectorizer\n",
    "# max_features = bag of words method, 1500 as most occuring words\n",
    "# mind_df = only include features that appear in at least 5 docs\n",
    "# max_df = like min_df, 0.7 means 70%\n",
    "# remove stopwords in file from nltk lib\n",
    "vtrizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "# assign number to words\n",
    "# fit_transform coverts text into num features\n",
    "X = vtrizer.fit_transform(tmp).toarray()\n",
    "\n",
    "print(len(X))\n",
    "for a in X[3]:\n",
    "    if a>0:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
